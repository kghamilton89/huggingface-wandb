


















































  3%|████▎                                                                                                                                    | 50/1608 [04:40<1:46:53,  4.12s/it]


















































  6%|████████▍                                                                                                                               | 100/1608 [08:18<1:50:36,  4.40s/it]


















































  9%|████████████▋                                                                                                                           | 150/1608 [11:34<1:43:32,  4.26s/it]


















































 12%|████████████████▉                                                                                                                       | 200/1608 [15:09<2:35:52,  6.64s/it]



















































 16%|█████████████████████▏                                                                                                                  | 250/1608 [18:35<1:19:34,  3.52s/it]


















































 19%|█████████████████████████▎                                                                                                              | 300/1608 [23:10<1:54:40,  5.26s/it]

















































 22%|█████████████████████████████▌                                                                                                          | 349/1608 [28:27<2:04:04,  5.91s/it]


















































 25%|█████████████████████████████████▋                                                                                                      | 399/1608 [32:50<1:32:48,  4.61s/it]



















































 28%|██████████████████████████████████████                                                                                                  | 450/1608 [36:52<1:14:55,  3.88s/it]

















































 31%|██████████████████████████████████████████▎                                                                                             | 500/1608 [41:00<1:46:05,  5.75s/it][INFO|trainer.py:3070] 2024-03-01 13:01:48,017 >> Saving model checkpoint to /tmp/COLA/tmp-checkpoint-500
[INFO|configuration_utils.py:471] 2024-03-01 13:01:48,052 >> Configuration saved in /tmp/COLA/tmp-checkpoint-500/config.json
{'loss': 0.2798, 'grad_norm': 10.397448539733887, 'learning_rate': 1.3781094527363185e-05, 'epoch': 1.87}
[INFO|modeling_utils.py:2454] 2024-03-01 13:01:48,640 >> Model weights saved in /tmp/COLA/tmp-checkpoint-500/model.safetensors
[INFO|tokenization_utils_base.py:2459] 2024-03-01 13:01:48,728 >> tokenizer config file saved in /tmp/COLA/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-01 13:01:48,728 >> Special tokens file saved in /tmp/COLA/tmp-checkpoint-500/special_tokens_map.json
















































 34%|██████████████████████████████████████████████▍                                                                                         | 549/1608 [45:42<2:10:43,  7.41s/it]



















































 37%|██████████████████████████████████████████████████▋                                                                                     | 600/1608 [50:56<1:43:03,  6.13s/it]

















































 40%|██████████████████████████████████████████████████████▉                                                                                 | 649/1608 [55:02<1:23:54,  5.25s/it]


















































 43%|██████████████████████████████████████████████████████████▎                                                                           | 699/1608 [1:06:03<1:07:29,  4.45s/it]


















































 47%|██████████████████████████████████████████████████████████████▍                                                                       | 749/1608 [1:10:55<1:24:04,  5.87s/it]


















































 50%|██████████████████████████████████████████████████████████████████▌                                                                   | 799/1608 [1:25:38<1:10:08,  5.20s/it]


















































 53%|██████████████████████████████████████████████████████████████████████▊                                                               | 849/1608 [1:29:57<1:01:49,  4.89s/it]


















































 56%|████████████████████████████████████████████████████████████████████████████                                                            | 899/1608 [1:34:36<55:29,  4.70s/it]


















































 59%|███████████████████████████████████████████████████████████████████████████████                                                       | 949/1608 [1:39:02<1:05:51,  6.00s/it]


















































 62%|██████████████████████████████████████████████████████████████████████████████████▋                                                  | 1000/1608 [1:43:30<1:09:06,  6.82s/it][INFO|trainer.py:3070] 2024-03-01 14:04:17,530 >> Saving model checkpoint to /tmp/COLA/tmp-checkpoint-1000
[INFO|configuration_utils.py:471] 2024-03-01 14:04:17,557 >> Configuration saved in /tmp/COLA/tmp-checkpoint-1000/config.json
[INFO|modeling_utils.py:2454] 2024-03-01 14:04:18,460 >> Model weights saved in /tmp/COLA/tmp-checkpoint-1000/model.safetensors
[INFO|tokenization_utils_base.py:2459] 2024-03-01 14:04:18,475 >> tokenizer config file saved in /tmp/COLA/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-01 14:04:18,475 >> Special tokens file saved in /tmp/COLA/tmp-checkpoint-1000/special_tokens_map.json
{'loss': 0.1418, 'grad_norm': 5.532305717468262, 'learning_rate': 7.5621890547263685e-06, 'epoch': 3.73}

















































 65%|████████████████████████████████████████████████████████████████████████████████████████▏                                              | 1050/1608 [1:47:59<45:42,  4.92s/it]

















































 68%|████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 1099/1608 [1:52:14<43:44,  5.16s/it]


















































 71%|████████████████████████████████████████████████████████████████████████████████████████████████▍                                      | 1149/1608 [1:56:18<42:12,  5.52s/it]


















































 75%|████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 1199/1608 [2:00:37<36:02,  5.29s/it]


















































 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 1249/1608 [2:04:50<31:05,  5.20s/it]


















































 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                          | 1299/1608 [2:08:58<24:31,  4.76s/it]



















































 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                     | 1350/1608 [2:13:22<20:41,  4.81s/it]


















































 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 1400/1608 [2:17:51<18:48,  5.42s/it]


















































 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋             | 1450/1608 [2:22:09<12:39,  4.81s/it]

















































 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉         | 1500/1608 [2:26:15<09:08,  5.08s/it][INFO|trainer.py:3070] 2024-03-01 14:47:02,081 >> Saving model checkpoint to /tmp/COLA/tmp-checkpoint-1500
[INFO|configuration_utils.py:471] 2024-03-01 14:47:02,108 >> Configuration saved in /tmp/COLA/tmp-checkpoint-1500/config.json
[INFO|modeling_utils.py:2454] 2024-03-01 14:47:02,660 >> Model weights saved in /tmp/COLA/tmp-checkpoint-1500/model.safetensors
[INFO|tokenization_utils_base.py:2459] 2024-03-01 14:47:02,686 >> tokenizer config file saved in /tmp/COLA/tmp-checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-01 14:47:02,686 >> Special tokens file saved in /tmp/COLA/tmp-checkpoint-1500/special_tokens_map.json
{'loss': 0.065, 'grad_norm': 0.13492147624492645, 'learning_rate': 1.3432835820895524e-06, 'epoch': 5.6}

















































 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏    | 1550/1608 [2:30:21<05:03,  5.24s/it]

















































 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 1599/1608 [2:34:10<00:42,  4.77s/it]








100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1607/1608 [2:34:46<00:04,  4.47s/it]
{'train_runtime': 9290.2367, 'train_samples_per_second': 5.523, 'train_steps_per_second': 0.173, 'train_loss': 0.2082608659654411, 'epoch': 6.0}
***** train metrics *****
  epoch                    =        6.0
  train_loss               =     0.2083
  train_runtime            = 2:34:50.23
  train_samples            =       8551
  train_samples_per_second =      5.523
  train_steps_per_second   =      0.173
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1608/1608 [2:34:48<00:00,  3.56s/it][INFO|trainer.py:2070] 2024-03-01 14:55:35,019 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1608/1608 [2:34:48<00:00,  5.78s/it]
[INFO|trainer.py:3070] 2024-03-01 14:55:35,140 >> Saving model checkpoint to /tmp/COLA/
[INFO|configuration_utils.py:471] 2024-03-01 14:55:35,150 >> Configuration saved in /tmp/COLA/config.json
[INFO|modeling_utils.py:2454] 2024-03-01 14:55:35,561 >> Model weights saved in /tmp/COLA/model.safetensors
[INFO|tokenization_utils_base.py:2459] 2024-03-01 14:55:35,579 >> tokenizer config file saved in /tmp/COLA/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-01 14:55:35,580 >> Special tokens file saved in /tmp/COLA/special_tokens_map.json
[INFO|trainer.py:762] 2024-03-01 14:55:35,667 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3379] 2024-03-01 14:55:35,711 >> ***** Running Evaluation *****
[INFO|trainer.py:3381] 2024-03-01 14:55:35,712 >>   Num examples = 1043
[INFO|trainer.py:3384] 2024-03-01 14:55:35,712 >>   Batch size = 8









 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 128/131 [00:18<00:00,  7.02it/s]
***** eval metrics *****
  epoch                     =        6.0
  eval_loss                 =     0.8142
  eval_matthews_correlation =     0.5634
  eval_runtime              = 0:00:19.70
  eval_samples              =       1043
  eval_samples_per_second   =     52.938

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 131/131 [00:19<00:00,  6.83it/s]